{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1KnkVuYW6Ne25hOZ_IApiv_MIYb4lxCAq","timestamp":1654184585259}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jTp0pIIqNs-M"},"source":["**Mounting Google-Drive:**\n","\n","First of all we must mount our drive"]},{"cell_type":"code","metadata":{"id":"_fpYcLygMlgI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654185386722,"user_tz":-330,"elapsed":25768,"user":{"displayName":"Syam Prakash","userId":"07562999416210785492"}},"outputId":"22d9e50d-4e0c-43ae-a369-9cd5c8490401"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"6K4S2s33ebY0"},"source":["**Install libraries:**\n"]},{"cell_type":"code","metadata":{"id":"1jUmnKBmNZ-t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654185410606,"user_tz":-330,"elapsed":22928,"user":{"displayName":"Syam Prakash","userId":"07562999416210785492"}},"outputId":"bfb93322-a450-4c94-e970-f83ced94e40c"},"source":["! rm -r unilm\n","! git clone -b remove_torch_save https://github.com/NielsRogge/unilm.git\n","! cd unilm/layoutlm\n","! pip install unilm/layoutlm"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove 'unilm': No such file or directory\n","Cloning into 'unilm'...\n","remote: Enumerating objects: 4248, done.\u001b[K\n","remote: Total 4248 (delta 0), reused 0 (delta 0), pack-reused 4248\u001b[K\n","Receiving objects: 100% (4248/4248), 7.29 MiB | 10.69 MiB/s, done.\n","Resolving deltas: 100% (2087/2087), done.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing ./unilm/layoutlm\n","\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n","   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n","Collecting transformers==2.9.0\n","  Downloading transformers-2.9.0-py3-none-any.whl (635 kB)\n","\u001b[K     |████████████████████████████████| 635 kB 7.1 MB/s \n","\u001b[?25hCollecting tensorboardX==2.0\n","  Downloading tensorboardX-2.0-py2.py3-none-any.whl (195 kB)\n","\u001b[K     |████████████████████████████████| 195 kB 63.1 MB/s \n","\u001b[?25hCollecting lxml==4.5.1\n","  Downloading lxml-4.5.1-cp37-cp37m-manylinux1_x86_64.whl (5.5 MB)\n","\u001b[K     |████████████████████████████████| 5.5 MB 10.4 MB/s \n","\u001b[?25hCollecting seqeval==0.0.12\n","  Downloading seqeval-0.0.12.tar.gz (21 kB)\n","Requirement already satisfied: Pillow==7.1.2 in /usr/local/lib/python3.7/dist-packages (from layoutlm==0.0) (7.1.2)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval==0.0.12->layoutlm==0.0) (1.21.6)\n","Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.7/dist-packages (from seqeval==0.0.12->layoutlm==0.0) (2.8.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.0->layoutlm==0.0) (1.15.0)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.0->layoutlm==0.0) (3.17.3)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 44.1 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0->layoutlm==0.0) (3.7.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0->layoutlm==0.0) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0->layoutlm==0.0) (4.64.0)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 57.0 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0->layoutlm==0.0) (2019.12.20)\n","Collecting tokenizers==0.7.0\n","  Downloading tokenizers-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.6 MB)\n","\u001b[K     |████████████████████████████████| 5.6 MB 22.3 MB/s \n","\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0->layoutlm==0.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0->layoutlm==0.0) (2022.5.18.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0->layoutlm==0.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0->layoutlm==0.0) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0->layoutlm==0.0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0->layoutlm==0.0) (1.1.0)\n","Building wheels for collected packages: layoutlm, seqeval, sacremoses\n","  Building wheel for layoutlm (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for layoutlm: filename=layoutlm-0.0-py3-none-any.whl size=11487 sha256=da960ba4b4df460234dc286eacbff4ef7f4801c3e8001ad19d0f38194ae18cd6\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-eg8niqjm/wheels/ce/83/11/541e5dad69eb41732abc7eaa2efde047a655aaae9c6df0fe15\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-0.0.12-py3-none-any.whl size=7435 sha256=586093d5f838f6127f2ab87386b6b42bf6bfffd4871c8c9da4849d4b92941ee9\n","  Stored in directory: /root/.cache/pip/wheels/dc/cc/62/a3b81f92d35a80e39eb9b2a9d8b31abac54c02b21b2d466edc\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=9ad6446b638ebea85ae8ea0c94fee4ccb7416d5490e204c1b1d4c1c1bb2c91e2\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built layoutlm seqeval sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers, tensorboardX, seqeval, lxml, layoutlm\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.12.1\n","    Uninstalling tokenizers-0.12.1:\n","      Successfully uninstalled tokenizers-0.12.1\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.20.0.dev0\n","    Uninstalling transformers-4.20.0.dev0:\n","      Successfully uninstalled transformers-4.20.0.dev0\n","  Attempting uninstall: lxml\n","    Found existing installation: lxml 4.2.6\n","    Uninstalling lxml-4.2.6:\n","      Successfully uninstalled lxml-4.2.6\n","Successfully installed layoutlm-0.0 lxml-4.5.1 sacremoses-0.0.53 sentencepiece-0.1.96 seqeval-0.0.12 tensorboardX-2.0 tokenizers-0.7.0 transformers-2.9.0\n"]}]},{"cell_type":"code","metadata":{"id":"oIjawmtEPY1N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654185501875,"user_tz":-330,"elapsed":29597,"user":{"displayName":"Syam Prakash","userId":"07562999416210785492"}},"outputId":"02704b01-d423-4659-dc56-7bfaf9f0cc96"},"source":["! rm -r transformers\n","! git clone https://github.com/huggingface/transformers.git\n","! cd transformers\n","! pip install ./transformers"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'transformers'...\n","remote: Enumerating objects: 97307, done.\u001b[K\n","remote: Counting objects: 100% (73/73), done.\u001b[K\n","remote: Compressing objects: 100% (56/56), done.\u001b[K\n","remote: Total 97307 (delta 31), reused 29 (delta 12), pack-reused 97234\n","Receiving objects: 100% (97307/97307), 89.99 MiB | 26.90 MiB/s, done.\n","Resolving deltas: 100% (71561/71561), done.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing ./transformers\n","\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n","   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.0.dev0) (0.12.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.0.dev0) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.0.dev0) (3.7.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.0.dev0) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.0.dev0) (2019.12.20)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.0.dev0) (6.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.0.dev0) (4.11.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.0.dev0) (4.64.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.0.dev0) (21.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.20.0.dev0) (0.7.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.20.0.dev0) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.20.0.dev0) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.20.0.dev0) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.20.0.dev0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.20.0.dev0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.20.0.dev0) (2022.5.18.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.20.0.dev0) (3.0.4)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.20.0.dev0-py3-none-any.whl size=4341198 sha256=13b9f31b70454e0cb9e05bac071a0d3522396385a3d5db43f4fcb9a601487525\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-6c3b3cp9/wheels/49/62/f4/6730819eed4e6468662b1519bf3bf46419b2335990c77f8767\n","Successfully built transformers\n","Installing collected packages: transformers\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.20.0.dev0\n","    Uninstalling transformers-4.20.0.dev0:\n","      Successfully uninstalled transformers-4.20.0.dev0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","layoutlm 0.0 requires transformers==2.9.0, but you have transformers 4.20.0.dev0 which is incompatible.\u001b[0m\n","Successfully installed transformers-4.20.0.dev0\n"]}]},{"cell_type":"code","metadata":{"id":"PIjINVaNRl8J"},"source":["from torch.nn import CrossEntropyLoss\n","\n","def get_labels(path):\n","    with open(path, \"r\") as f:\n","        labels = f.read().splitlines()\n","    if \"O\" not in labels:\n","        labels = [\"O\"] + labels\n","    return labels\n","\n","labels = get_labels(\"/content/drive/MyDrive/UBIAI_layoutlm/labels.txt\")\n","num_labels = len(labels)\n","label_map = {i: label for i, label in enumerate(labels)}\n","# Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n","pad_token_label_id = CrossEntropyLoss().ignore_index"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9cBBA_ws0rTZ"},"source":["from transformers import LayoutLMTokenizer\n","from layoutlm.data.funsd import FunsdDataset, InputFeatures\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","args = {'local_rank': -1,\n","        'overwrite_cache': True,\n","        'data_dir': '/content/drive/MyDrive/UBIAI_layoutlm/',\n","        'model_name_or_path':'microsoft/layoutlm-base-uncased',\n","        'max_seq_length': 512,\n","        'model_type': 'layoutlm',}\n","\n","# class to turn the keys of a dict into attributes (thanks Stackoverflow)\n","class AttrDict(dict):\n","    def __init__(self, *args, **kwargs):\n","        super(AttrDict, self).__init__(*args, **kwargs)\n","        self.__dict__ = self\n","\n","args = AttrDict(args)\n","\n","tokenizer = LayoutLMTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n","\n","# the LayoutLM authors already defined a specific FunsdDataset, so we are going to use this here\n","train_dataset = FunsdDataset(args, tokenizer, labels, pad_token_label_id, mode=\"train\")\n","train_sampler = RandomSampler(train_dataset)\n","train_dataloader = DataLoader(train_dataset,\n","                              sampler=train_sampler,\n","                              batch_size=2)\n","\n","eval_dataset = FunsdDataset(args, tokenizer, labels, pad_token_label_id, mode=\"test\")\n","eval_sampler = SequentialSampler(eval_dataset)\n","eval_dataloader = DataLoader(eval_dataset,\n","                             sampler=eval_sampler,\n","                            batch_size=2)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8IzfppTr5EBJ"},"source":["batch = next(iter(train_dataloader))\n","input_ids = batch[0][0]\n","tokenizer.decode(input_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GSOZ3OSp5Ndq"},"source":["from transformers import LayoutLMForTokenClassification\n","import torch\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = LayoutLMForTokenClassification.from_pretrained(\"microsoft/layoutlm-base-uncased\", num_labels=num_labels)\n","model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9kG_NGIu30MR"},"source":["\n","from transformers import LayoutLMTokenizer\n","from layoutlm.data.funsd import FunsdDataset, InputFeatures\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","\n","tokenizer = LayoutLMTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kij9G--Z6kdD"},"source":["**Start Training**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FZzU-3tG5oER","executionInfo":{"status":"ok","timestamp":1630100057421,"user_tz":-60,"elapsed":149174,"user":{"displayName":"Saifeddine Trabelsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh47_1gli10ddIP-InuYoIWLm7G7Qv1hEDyg_suYQ=s64","userId":"13958221260964589467"}},"outputId":"af959aff-1550-4a5a-ba7e-0a82a665cc50"},"source":["from transformers import AdamW\n","from tqdm import tqdm\n","\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","\n","global_step = 0\n","num_train_epochs = 15\n","t_total = len(train_dataloader) * num_train_epochs # total number of training steps\n","\n","#put the model in training mode\n","model.train()\n","for epoch in range(num_train_epochs):\n","  for batch in tqdm(train_dataloader, desc=\"Training\"):\n","      input_ids = batch[0].to(device)\n","      bbox = batch[4].to(device)\n","      attention_mask = batch[1].to(device)\n","      token_type_ids = batch[2].to(device)\n","      labels = batch[3].to(device)\n","\n","      # forward pass\n","      outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids,\n","                      labels=labels)\n","      loss = outputs.loss\n","\n","      # print loss every 100 steps\n","      if global_step % 100 == 0:\n","        print(f\"Loss after {global_step} steps: {loss.item()}\")\n","\n","      # backward pass to get the gradients\n","      loss.backward()\n","\n","      #print(\"Gradients on classification head:\")\n","      #print(model.classifier.weight.grad[6,:].sum())\n","\n","      # update\n","      optimizer.step()\n","      optimizer.zero_grad()\n","      global_step += 1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\rTraining:   0%|          | 0/23 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Loss after 0 steps: 3.6813127994537354\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 23/23 [00:10<00:00,  2.29it/s]\n","Training: 100%|██████████| 23/23 [00:09<00:00,  2.35it/s]\n","Training: 100%|██████████| 23/23 [00:09<00:00,  2.34it/s]\n","Training: 100%|██████████| 23/23 [00:09<00:00,  2.32it/s]\n","Training:  35%|███▍      | 8/23 [00:03<00:06,  2.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["Loss after 100 steps: 0.5740603804588318\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 23/23 [00:09<00:00,  2.32it/s]\n","Training: 100%|██████████| 23/23 [00:09<00:00,  2.32it/s]\n","Training: 100%|██████████| 23/23 [00:09<00:00,  2.31it/s]\n","Training: 100%|██████████| 23/23 [00:09<00:00,  2.31it/s]\n","Training:  70%|██████▉   | 16/23 [00:06<00:03,  2.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Loss after 200 steps: 0.47880759835243225\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 23/23 [00:09<00:00,  2.31it/s]\n","Training: 100%|██████████| 23/23 [00:09<00:00,  2.30it/s]\n","Training: 100%|██████████| 23/23 [00:09<00:00,  2.31it/s]\n","Training: 100%|██████████| 23/23 [00:09<00:00,  2.31it/s]\n","Training: 100%|██████████| 23/23 [00:09<00:00,  2.31it/s]\n","Training:   4%|▍         | 1/23 [00:00<00:09,  2.41it/s]"],"name":"stderr"},{"output_type":"stream","text":["Loss after 300 steps: 0.05267507955431938\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 23/23 [00:09<00:00,  2.30it/s]\n","Training: 100%|██████████| 23/23 [00:09<00:00,  2.31it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"sNsh67yn5XFb"},"source":["## Evaluation\n","\n","Now let's evaluate on the test set:"]},{"cell_type":"code","metadata":{"id":"t6lRUS245sIi"},"source":["import numpy as np\n","from seqeval.metrics import (\n","    classification_report,\n","    f1_score,\n","    precision_score,\n","    recall_score,\n",")\n","\n","eval_loss = 0.0\n","nb_eval_steps = 0\n","preds = None\n","out_label_ids = None\n","\n","# put model in evaluation mode\n","model.eval()\n","for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","    with torch.no_grad():\n","        input_ids = batch[0].to(device)\n","        bbox = batch[4].to(device)\n","        attention_mask = batch[1].to(device)\n","        token_type_ids = batch[2].to(device)\n","        labels = batch[3].to(device)\n","\n","        # forward pass\n","        outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids,\n","                        labels=labels)\n","        # get the loss and logits\n","        tmp_eval_loss = outputs.loss\n","        logits = outputs.logits\n","\n","        eval_loss += tmp_eval_loss.item()\n","        nb_eval_steps += 1\n","\n","        # compute the predictions\n","        if preds is None:\n","            preds = logits.detach().cpu().numpy()\n","            out_label_ids = labels.detach().cpu().numpy()\n","        else:\n","            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n","            out_label_ids = np.append(\n","                out_label_ids, labels.detach().cpu().numpy(), axis=0\n","            )\n","\n","# compute average evaluation loss\n","eval_loss = eval_loss / nb_eval_steps\n","preds = np.argmax(preds, axis=2)\n","\n","out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n","preds_list = [[] for _ in range(out_label_ids.shape[0])]\n","\n","for i in range(out_label_ids.shape[0]):\n","    for j in range(out_label_ids.shape[1]):\n","        if out_label_ids[i, j] != pad_token_label_id:\n","            out_label_list[i].append(label_map[out_label_ids[i][j]])\n","            preds_list[i].append(label_map[preds[i][j]])\n","\n","results = {\n","    \"loss\": eval_loss,\n","    \"precision\": precision_score(out_label_list, preds_list),\n","    \"recall\": recall_score(out_label_list, preds_list),\n","    \"f1\": f1_score(out_label_list, preds_list),\n","}\n","print(results)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hhZWZhc_ZHlv"},"source":["!sudo apt install tesseract-ocr\n","!pip install pytesseract"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eQ4HALkUjGtf"},"source":["import pytesseract\n","from PIL import Image, ImageDraw, ImageFont\n","\n","#image = Image.open('/content/form_example.jpg')\n","image = Image.open(\"/content/drive/MyDrive/FACTURE_2021-06-10.jpg\")\n","image = image.convert(\"RGB\")\n","image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Czq7m1Exji1r"},"source":["import numpy as np\n","\n","width, height = image.size\n","w_scale = 1000/width\n","h_scale = 1000/height\n","\n","ocr_df = pytesseract.image_to_data(image, output_type='data.frame') \\\n","\n","ocr_df = ocr_df.dropna() \\\n","               .assign(left_scaled = ocr_df.left*w_scale,\n","                       width_scaled = ocr_df.width*w_scale,\n","                       top_scaled = ocr_df.top*h_scale,\n","                       height_scaled = ocr_df.height*h_scale,\n","                       right_scaled = lambda x: x.left_scaled + x.width_scaled,\n","                       bottom_scaled = lambda x: x.top_scaled + x.height_scaled)\n","\n","float_cols = ocr_df.select_dtypes('float').columns\n","ocr_df[float_cols] = ocr_df[float_cols].round(0).astype(int)\n","ocr_df = ocr_df.replace(r'^\\s*$', np.nan, regex=True)\n","ocr_df = ocr_df.dropna().reset_index(drop=True)\n","ocr_df[:20]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yH6AwLdEjnLu","executionInfo":{"status":"ok","timestamp":1630100457746,"user_tz":-60,"elapsed":501,"user":{"displayName":"Saifeddine Trabelsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh47_1gli10ddIP-InuYoIWLm7G7Qv1hEDyg_suYQ=s64","userId":"13958221260964589467"}},"outputId":"cdbb0e2d-1c5e-47ee-e349-77123d59f3bc"},"source":["len(ocr_df)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["305"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"0wkz6sqhjwU7"},"source":["words = list(ocr_df.text)\n","coordinates = ocr_df[['left', 'top', 'width', 'height']]\n","actual_boxes = []\n","for idx, row in coordinates.iterrows():\n","  x, y, w, h = tuple(row) # the row comes in (left, top, width, height) format\n","  actual_box = [x, y, x+w, y+h] # we turn it into (left, top, left+widght, top+height) to get the actual box\n","  actual_boxes.append(actual_box)\n","\n","def normalize_box(box, width, height):\n","    return [\n","        int(1000 * (box[0] / width)),\n","        int(1000 * (box[1] / height)),\n","        int(1000 * (box[2] / width)),\n","        int(1000 * (box[3] / height)),\n","    ]\n","\n","boxes = []\n","for box in actual_boxes:\n","  boxes.append(normalize_box(box, width, height))\n","boxes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wqyAeMI9j105"},"source":["def convert_example_to_features(image, words, boxes, actual_boxes, tokenizer, args, cls_token_box=[0, 0, 0, 0],\n","                                 sep_token_box=[1000, 1000, 1000, 1000],\n","                                 pad_token_box=[0, 0, 0, 0]):\n","      width, height = image.size\n","\n","      tokens = []\n","      token_boxes = []\n","      actual_bboxes = [] # we use an extra b because actual_boxes is already used\n","      token_actual_boxes = []\n","      for word, box, actual_bbox in zip(words, boxes, actual_boxes):\n","          word_tokens = tokenizer.tokenize(word)\n","          tokens.extend(word_tokens)\n","          token_boxes.extend([box] * len(word_tokens))\n","          actual_bboxes.extend([actual_bbox] * len(word_tokens))\n","          token_actual_boxes.extend([actual_bbox] * len(word_tokens))\n","\n","      # Truncation: account for [CLS] and [SEP] with \"- 2\".\n","      special_tokens_count = 2\n","      if len(tokens) > args.max_seq_length - special_tokens_count:\n","          tokens = tokens[: (args.max_seq_length - special_tokens_count)]\n","          token_boxes = token_boxes[: (args.max_seq_length - special_tokens_count)]\n","          actual_bboxes = actual_bboxes[: (args.max_seq_length - special_tokens_count)]\n","          token_actual_boxes = token_actual_boxes[: (args.max_seq_length - special_tokens_count)]\n","\n","      # add [SEP] token, with corresponding token boxes and actual boxes\n","      tokens += [tokenizer.sep_token]\n","      token_boxes += [sep_token_box]\n","      actual_bboxes += [[0, 0, width, height]]\n","      token_actual_boxes += [[0, 0, width, height]]\n","\n","      segment_ids = [0] * len(tokens)\n","\n","      # next: [CLS] token\n","      tokens = [tokenizer.cls_token] + tokens\n","      token_boxes = [cls_token_box] + token_boxes\n","      actual_bboxes = [[0, 0, width, height]] + actual_bboxes\n","      token_actual_boxes = [[0, 0, width, height]] + token_actual_boxes\n","      segment_ids = [1] + segment_ids\n","\n","      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n","      # tokens are attended to.\n","      input_mask = [1] * len(input_ids)\n","\n","      # Zero-pad up to the sequence length.\n","      padding_length = args.max_seq_length - len(input_ids)\n","      input_ids += [tokenizer.pad_token_id] * padding_length\n","      input_mask += [0] * padding_length\n","      segment_ids += [tokenizer.pad_token_id] * padding_length\n","      token_boxes += [pad_token_box] * padding_length\n","      token_actual_boxes += [pad_token_box] * padding_length\n","\n","      assert len(input_ids) == args.max_seq_length\n","      assert len(input_mask) == args.max_seq_length\n","      assert len(segment_ids) == args.max_seq_length\n","      #assert len(label_ids) == args.max_seq_length\n","      assert len(token_boxes) == args.max_seq_length\n","      assert len(token_actual_boxes) == args.max_seq_length\n","\n","      return input_ids, input_mask, segment_ids, token_boxes, token_actual_boxes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YJO3J9qWj8U0"},"source":["input_ids, input_mask, segment_ids, token_boxes, token_actual_boxes = convert_example_to_features(image=image, words=words, boxes=boxes, actual_boxes=actual_boxes, tokenizer=tokenizer, args=args)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":188},"id":"QcvxUcBekACh","executionInfo":{"status":"ok","timestamp":1630100533401,"user_tz":-60,"elapsed":375,"user":{"displayName":"Saifeddine Trabelsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh47_1gli10ddIP-InuYoIWLm7G7Qv1hEDyg_suYQ=s64","userId":"13958221260964589467"}},"outputId":"541e9476-ee0d-4a81-e723-1c4e11fd7e87"},"source":["tokenizer.decode(input_ids)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"[CLS] lonos : a mme. faten amamou 102 avenue des champs elysees 75008 paris france hebergement web pro 1 & 1 ionos sarl service comptable 7, place de la gare bp 70109 57201 sarreguemines cedex facture du : 22. 05. 2021 n° de facture : 202522025862 n° de contrat : 86662335 n° client : 507553341 centre d'assistance : ionos. fr / assistance my ionos : my. ionos. fr / invoices numero de telephone : 0970 808 911 adresse email : info - facturation @ ionos. fr horaires d'ouverture : 7j / 7, 24h / 24 pour une authentification sdre et rapide, nous vous prions de bien vouloir preparer votre pin telephonique afin de pouvoir nous le communiquer lors de votre appel. vous pouvez configurer et gerer ce dernier sur my. ionos. fr. pos. prestation tarif quantite prixht ( eur ) tva ( % ) 1 frais d'abonnement 8, 00 eur ht par mois 1 mois 8, 00 20, 00 21. 05. 2021 - 21. 06. 2021 2 offre speciale offre speciale - 7, 00 20, 00 reduction sur la position 1 valide de 21 / 02 / 2021 a 21 / 02 / 2022 somme intermediaire ( ht ) 1, 00 eur + tva ( 20, 00 % ) 0, 20 eur montant a payer ( ttc ) 1, 20 eur le montant total dd sera preleve sur votre carte de credit d'ici les 7 prochains jours a compter de la date de la presente facture. pour plus d'informations, veuillez - vous referer au centre d'assistance ou rendez - vous sur my. ionos. fr professionnels tout retard de paiement entraine de plein droit des penalites de retard a hauteur de 1, 5 fois le taux d ’ interet legal annuel, par mois de retard [SEP]\""]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3JNDRJPokC1H","executionInfo":{"status":"ok","timestamp":1630100559049,"user_tz":-60,"elapsed":525,"user":{"displayName":"Saifeddine Trabelsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh47_1gli10ddIP-InuYoIWLm7G7Qv1hEDyg_suYQ=s64","userId":"13958221260964589467"}},"outputId":"32cf9ef3-d057-49f9-b324-356d3d35035b"},"source":["input_ids = torch.tensor(input_ids, device=device).unsqueeze(0)\n","input_ids.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 512])"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MtxjFuU5kJDj","executionInfo":{"status":"ok","timestamp":1630100569076,"user_tz":-60,"elapsed":684,"user":{"displayName":"Saifeddine Trabelsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh47_1gli10ddIP-InuYoIWLm7G7Qv1hEDyg_suYQ=s64","userId":"13958221260964589467"}},"outputId":"edb29309-43b2-485e-a0d0-0db4104a1f15"},"source":["attention_mask = torch.tensor(input_mask, device=device).unsqueeze(0)\n","attention_mask.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 512])"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e26jBeIrkLd7","executionInfo":{"status":"ok","timestamp":1630100579681,"user_tz":-60,"elapsed":518,"user":{"displayName":"Saifeddine Trabelsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh47_1gli10ddIP-InuYoIWLm7G7Qv1hEDyg_suYQ=s64","userId":"13958221260964589467"}},"outputId":"b7f48e9d-9b48-4078-95a2-dc22988f8b2c"},"source":["token_type_ids = torch.tensor(segment_ids, device=device).unsqueeze(0)\n","token_type_ids.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 512])"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pHJQ0c2UkOGK","executionInfo":{"status":"ok","timestamp":1630100589332,"user_tz":-60,"elapsed":370,"user":{"displayName":"Saifeddine Trabelsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh47_1gli10ddIP-InuYoIWLm7G7Qv1hEDyg_suYQ=s64","userId":"13958221260964589467"}},"outputId":"fdda89b4-38c8-4379-ee88-8ad83836f957"},"source":["bbox = torch.tensor(token_boxes, device=device).unsqueeze(0)\n","bbox.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 512, 4])"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"BOiZNMopkQfV"},"source":["outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yX9o99xEkT0p"},"source":["outputs.logits.shape\n","print(outputs.logits)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FjKC7AaVkYVx"},"source":["outputs.logits.argmax(-1)\n","print(outputs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Yueph4HkcDJ"},"source":["token_predictions = outputs.logits.argmax(-1).squeeze().tolist() # the predictions are at the token level\n","print(token_predictions)\n","\n","word_level_predictions = [] # let's turn them into word level predictions\n","final_boxes = []\n","for id, token_pred, box in zip(input_ids.squeeze().tolist(), token_predictions, token_actual_boxes):\n","  if (tokenizer.decode([id]).startswith(\"##\")) or (id in [tokenizer.cls_token_id,\n","                                                           tokenizer.sep_token_id,\n","                                                          tokenizer.pad_token_id]):\n","    # skip prediction + bounding box\n","\n","    continue\n","  else:\n","    word_level_predictions.append(token_pred)\n","    final_boxes.append(box)\n","\n","# for id, prediction in zip(input_ids.squeeze().tolist(), predictions):\n","#   if id != 0:\n","#     print(tokenizer.decode([id]), label_map[prediction])\n","print(word_level_predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"laYonDf6kg09"},"source":["print(len(word_level_predictions))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jQ--4-oQkl6n","executionInfo":{"status":"ok","timestamp":1630100689676,"user_tz":-60,"elapsed":8,"user":{"displayName":"Saifeddine Trabelsi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh47_1gli10ddIP-InuYoIWLm7G7Qv1hEDyg_suYQ=s64","userId":"13958221260964589467"}},"outputId":"119c20ea-bba0-42b3-874a-b1f7268b48b4"},"source":["print(len(final_boxes))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["339\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WT69kxNSkobc"},"source":["draw = ImageDraw.Draw(image)\n","\n","font = ImageFont.load_default()\n","\n","def iob_to_label(label):\n","  if label != 'O':\n","    return label[2:]\n","  else:\n","    return \"\"\n","\n","label2color = {'question':'blue', 'answer':'green', 'header':'orange', 'other':'violet'}\n","\n","for prediction, box in zip(word_level_predictions, final_boxes):\n","    predicted_label = iob_to_label(label_map[prediction]).lower()\n","    # print(word_level_predictions)\n","    # print(label_map[prediction])\n","    #print('equal to other')\n","    draw.rectangle(box, outline='blue')\n","    draw.text((box[0] + 10, box[1] - 10), text=predicted_label, fill='blue', font=font)\n","\n","image"],"execution_count":null,"outputs":[]}]}